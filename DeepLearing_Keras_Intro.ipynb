{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras Functional API\n",
    "keras is a deep learning API that can run on a computation back such as tensor flow or theano.  This allows a much more readable system for creating, traning and deploying \n",
    "+ Keras has two API (functional and sequential)\n",
    "This into uses the functional API\n",
    "\n",
    "##### Creating a keras model\n",
    "Keras models are comprised of the following components\n",
    "\n",
    "+ Architecture:\n",
    "    + Inputs\n",
    "    + Layers\n",
    "        + Initializers \n",
    "        + Activation Functions\n",
    "    + Outputs\n",
    "\n",
    "+ Loss Function (how to calculated error when the model is training for each row )\n",
    "+ Optimizer (how weights are adjusted after the error is calculated)\n",
    "+ Metrics (batch wise  over all statistics during training)\n",
    "+ Callbacks (extra steps during training to monitor progress optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\Lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((426, 30), (426,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "# loads a data set\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(2012)\n",
    "bc = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(bc.data, bc.target, random_state=2012)\n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Inputs Using Keras Input Layer\n",
    "\n",
    "keras Input shape as an argument where shape tuple of integers. This is tpyically refered to as the 'input_layer'\n",
    "\n",
    "+ 2D, (rows, columns)  for data with num_samples and n_features, ```Input(shape=( n_features,))```\n",
    "\n",
    "    This means the the network is expecting an array with shape (num_samples,  n_features), each batch will be a subset of rows\n",
    "\n",
    "\n",
    "+ 3D Inputs such as time sequences ```Input(shape=(time_steps, n_features)```\n",
    "\n",
    "    This means the model would expect input of the shape(n_samples, time_steps, n_features)\n",
    "    an example of this would be a data set 10 sequential observations of pulse and temp for 50 patients would have shape\n",
    "    (50, 10, 2), batches would be a subset of patients\n",
    "\n",
    "\n",
    "+ 4D Data Such as Images ```Input(shape=(width, height, n_color_channels))```\n",
    "The data input expect for this scheme would be of the shape (n_samples, width, height, n_color_channels) where the batches are taken from a subset of sample images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of columns of the input data\n",
    "n_input_cols = X_train.shape[1]\n",
    "\n",
    "# sets up the input shape expectation for the model\n",
    "inputs = Input(shape=(n_input_cols,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers \n",
    "There are many types of keras layers. The most common is a Dense Layer.\n",
    "+ BatchNormalizationLayer: takes data from previous layer, try to keep mean 0 stdev 1 \n",
    "    + maintains weights that are trainable for this transformation\n",
    "    + takes input from previous layer, attempts to scale each input it to mean 0 stdev 1\n",
    "\n",
    "+ Dense Layers have a number of nodes, \n",
    "   +  ``` Dense(5)```  uses default intializer for weights and bias, has 5 hidden nodes\n",
    "    + maintains weights, and biases that are trainable\n",
    "    + multiplies a different wieght for each input for each node, add bias and sums\n",
    "    + takes that value and puts it through an activation function, such as Relu or TANH\n",
    "    \n",
    "##### Initializers\n",
    "In a dense layer, the weights (kernal) need to be initialized some how, either choosen from a random normal distrobution at random, set to all zeros or most typically choosen from the 'glorot_uniform' distrobution\n",
    "\n",
    "##### Activation Functions\n",
    "Activation functions are applied at each node to additionally transform the data.  The most typical is relu {x: x <0 then 0 else x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add normalization layer mean activation close to 0 and the activation standard deviation close to 1.\n",
    "normalization_layer = BatchNormalization()(inputs)\n",
    "\n",
    "# add a hidden Layer with 5 hidden notes\n",
    "hidden_layer = Dense(5, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros')( normalization_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs \n",
    "The output layer of the network is typically just another Dense Layer with an activation function that matches your prediction problem\n",
    "+ Number of nodes must match number of columns of your outcomes\n",
    "\n",
    "##### Classication\n",
    " + Classication where lables are an array of integer classes, is [1,0,1,0, 0 ..]\n",
    "      + ```outputs = Dense(1, activation='sigmoid')(hidden_layer)```\n",
    "      + when calling model.compile, use ```loss = 'binary_crossentropy'```\n",
    " + Classication where labels are one hot encododed [[1, 0], [0,1], ...]\n",
    "      + ```outputs = Dense(num_label_columns, activation='sigmoid')(hidden_layer)```\n",
    "      + when calling model.compile, use ```loss = 'categorical_crossentropy'```\n",
    " + Mulit Class Classication where labels are a list of integer classes [1,2,0,4, ... ]\n",
    "      + ```outputs = Dense(num_classes, activation='sigmoid')(hidden_layer)```\n",
    "      + when calling model.compile, use ```loss = 'sparse_categorical_crossentropy'```\n",
    " + Activation:\n",
    "     + 'sigmoid': when actually probabilities are important\n",
    "     + 'softmax': when only classication accuracy is important\n",
    "     \n",
    "##### Regression\n",
    "  + ```outputs = Dense(num_output_columns, activation='linear')(hidden_layer)```\n",
    "  + when calling model.compile, use ```loss = 'mean_squared_error'``` or similiar \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create ouput layer of the model  \n",
    "outputs = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compline the model meas assembling it together and defining a loss function and optimizer and any callbacks\n",
    "+ Optimizer is the method of that adjusts each weight given the Loss calcuated at the end of each batch\n",
    "    + typicaly choose 'adam' or 'Nadam'\n",
    "    \n",
    "+ Loss Function is how the model will calculated error for each sample, entropy based for classication and error based for the regression \n",
    "+ Metrics are calculated on batches of data (like AUC or correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 155       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 281\n",
      "Trainable params: 221\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model \n",
    "esm = EarlyStopping(monitor='val_loss', min_delta=0, patience=0)\n",
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Backs \n",
    "Call backs are esentially methods that run after each step.  The could send the results to tensorboard, save and check point the model, or in this case stop training if the model fails to improve\n",
    "+ EarlyStopping monitors the loss function, if it fails to improve in 2 batches, the model stops training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm = EarlyStopping(monitor='loss', min_delta=0, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "the .fit method is call on a X,y\n",
    "+ batch size is the number of rows sampled from x used to calcuated loss, and adjust weights\n",
    "+ validation_data is a tuple of (X,y) test data in which metrics and loss are callculated during each step\n",
    "+ epochs is the number of passed through the entire data set\n",
    "+ callbacks are a list of callback objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/10\n",
      "426/426 [==============================] - 0s 472us/step - loss: 0.1159 - acc: 0.9601 - val_loss: 0.1406 - val_acc: 0.9510\n",
      "Epoch 2/10\n",
      "426/426 [==============================] - 0s 221us/step - loss: 0.0989 - acc: 0.9648 - val_loss: 0.1389 - val_acc: 0.9510\n",
      "Epoch 3/10\n",
      "426/426 [==============================] - 0s 227us/step - loss: 0.1107 - acc: 0.9531 - val_loss: 0.1383 - val_acc: 0.9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227347446d8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),  batch_size=32, epochs=10,  callbacks=[esm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
